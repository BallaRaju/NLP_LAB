{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0a672a-79f2-42d9-9cf2-357044a38479",
   "metadata": {},
   "source": [
    "# Exercise – 1 \n",
    "\n",
    "## Problem Statement :-- \n",
    "## To demonstrate Noise Removal for any textual data and remove regular expression pattern such as hashtag from textual data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c605f4f-7550-4cf7-9438-06eb3b5cbabf",
   "metadata": {},
   "source": [
    "## Theoritical Background :-\n",
    "\n",
    "### Noise:-\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "For example – language stopwords (commonly used words of a language – is, am, the, of, in etc).\n",
    "\n",
    "URLs or links, social media entities (mentions, hashtags).\n",
    "\n",
    "punctuations and industry specific words.\n",
    "\n",
    "### Noise Removal:-\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.\n",
    "\n",
    "### Noise Removal through Regular Expressions\n",
    "\n",
    "Another approach is to use regular expressions while dealing with special patterns of noise.By importing re module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b5b55-62d3-409c-81d6-60237eeab292",
   "metadata": {},
   "source": [
    "###  Program to remove noisy words from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10435699-178b-4e37-8ad8-76657e429048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text\n"
     ]
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"]\n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_list]\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text\n",
    "print(_remove_noise(\"this is a sample text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8ef76-8a13-4374-a03b-09febf19d6cd",
   "metadata": {},
   "source": [
    "### Program to remove a regular expression pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2425bc09-d5e4-42b7-beaf-993d1d529bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be85887-81fd-45a9-a6a7-8b1a9534ed9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics 4idhya'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_regex(input_text, regex_pattern):\n",
    "    url = re.finditer(regex_pattern, input_text)\n",
    "    for i in url:\n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "regex_pattern = \"#[\\w]*\"\n",
    "remove_regex(\"remove this #hashtag from analytics 4idhya\", regex_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239cf99-c009-42b7-a500-20845fb98177",
   "metadata": {},
   "source": [
    "# Exercise – 2\n",
    "\n",
    "## Problem Statement :-\n",
    "## To perform lemmatization and stemming using python library nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4eb9d-509a-48af-a65d-e62ef868b567",
   "metadata": {},
   "source": [
    "## Theoritical background:-\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "For example – \"play\",\"player\",\"played\",\"plays\" and \"playing\" are the different variations of the word – \"play\".\n",
    "\n",
    "Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form also \n",
    "known as lemma. Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal task for any ML model.\n",
    "\n",
    "### Instructions:-\n",
    "\n",
    "Step-By-Step process ,\n",
    "\n",
    "Noise removal, Tokenization, and lemmatization or stemming to change the string into normalized form.\n",
    "\n",
    "### Tokenization:-\n",
    "\n",
    "In natural language processing, tokenization is the text preprocessing task of breaking up text into smaller components of text known as tokens.\n",
    "\n",
    "### Stemming:-\n",
    "\n",
    "In natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word prefixes and suffixes.\n",
    "\n",
    "If the text is not in tokens, then we need to convert it into tokens. After we have converted strings of text into tokens, we can convert the word tokens into their root form. There are mainly three algorithms for stemming. These are the Porter Stemmer, the Snowball Stemmer and the Lancaster Stemmer. Porter Stemmer is the most common among them.\n",
    "\n",
    "### Lemmatization:-\n",
    "\n",
    "In natural language processing, lemmatization is the text preprocessing normalization task concerned with bringing words down to their root forms.\n",
    "\n",
    "The word “Lemmatization” is itself made of the base word “Lemma”. In Linguistics (a field of study on which NLP is based) a lemma is a meaningful base word or a root word that forms the basis for other words. For example, the lemma of the words “playing” and “played” is “play”.\n",
    "\n",
    "### POS Tagging\n",
    "\n",
    "POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context. It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also called grammatical tagging.\n",
    "\n",
    "Let’s learn with a NLTK Part of Speech example:\n",
    "\n",
    "Input: Everything to permit us.\n",
    "\n",
    "Output: [(‘Everything’, NN),(‘to’, TO), (‘permit’, VB), (‘us’, PRP)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d88a1-4a21-4cca-a3df-cdb3859fb9e7",
   "metadata": {},
   "source": [
    "###  Program to demonstrate Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71b6c7-8e59-4ae2-aca0-d7891a492ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ead022-c945-4fb4-acf6-a8c720d54d7a",
   "metadata": {},
   "source": [
    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8131f79-4eb7-4a69-8fe8-f66355006b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "tokenized = [\"So\", \"many\", \"squids\", \"are\", \"jumping\"]\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf5ed7-989c-4fc4-b381-e051bb71c80c",
   "metadata": {},
   "source": [
    "['so', 'mani', 'squid', 'are', 'jump']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df048fdb-1479-42e0-8ad0-064b9e95e4f1",
   "metadata": {},
   "source": [
    "###  Program to demonstrate Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021aa18b-75f4-43d4-8223-5f60400f6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2beb4-bf7d-450b-9e36-40c7cb5ca4b4",
   "metadata": {},
   "source": [
    "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b18d51-98f4-4375-8a9f-20a95983b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc87a6-5788-4a0a-954a-85124c38d653",
   "metadata": {},
   "source": [
    "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
    "\n",
    "The striped bat are hanging on their foot for best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734ac6b-b2b1-4d8f-a390-19bc9266e223",
   "metadata": {},
   "source": [
    "###  Program to demonstrate Lemmatization with POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04227787-0c2b-4faa-a4fc-6e3ab19ddef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befad84-f125-4f5e-8e8f-d0f685bf83c2",
   "metadata": {},
   "source": [
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "\n",
    "[nltk_data]     /root/nltk_data...\n",
    "\n",
    "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a2e30-0339-43e0-82db-456e80a6d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "# Return wordnet.NOUN is given key not present in the dictionary\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([get_wordnet_pos(w) for w in nltk.word_tokenize(sentence)])\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb984aa-90cf-4605-908b-c63eb6eac5b7",
   "metadata": {},
   "source": [
    "foot\n",
    "\n",
    "['n', 'v', 'n', 'v', 'v', 'n', 'n', 'n', 'n', 'a']\n",
    "\n",
    "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5d627-4dff-4b9c-b2fd-e53656902389",
   "metadata": {},
   "source": [
    "# Exercise – 3\n",
    "\n",
    "## Problem Statement:-\n",
    "\n",
    "## Demonstrate object standardization such as replacing social media slangs from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98cff5-c12a-4a19-b17c-ec11eecfba16",
   "metadata": {},
   "source": [
    "## Theoritical background:-\n",
    "\n",
    "### Standardization:-\n",
    "\n",
    "Object standardization is pre-processing techniques that can be done on abbreviations such as “rt → retweet, dm → direct message”. Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16220c4a-cbc7-457d-861e-da4f44228916",
   "metadata": {},
   "source": [
    "###  Program to demonstrate Lemmatization with POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e3e48d-b4ac-42a7-9a70-569ad6b12a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Shivam Bansal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {\"rt\":\"Retweet\", \"dm\":\"direct message\", \"awsm\" : \"awesome\",\"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "        new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249191e-8621-43aa-8c60-fd509ba993a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
